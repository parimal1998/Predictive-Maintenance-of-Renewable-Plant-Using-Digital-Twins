{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parimal1998/Predictive-Maintenance-of-Renewable-Plant-Using-Digital-Twins/blob/main/RNN_Model1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os, math, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv1D, Bidirectional, LSTM, Dense, Dropout, LayerNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "pLSyDMOBzmzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "CANDIDATE_PATHS = [\n",
        "    '/content/Final Dataset.csv',\n",
        "]\n",
        "csv_path = None\n",
        "for p in CANDIDATE_PATHS:\n",
        "    if os.path.exists(p):\n",
        "        csv_path = p\n",
        "        break\n",
        "assert csv_path is not None, \"Could not find 'Final Dataset.csv' in known locations.\"\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "assert 'time' in df.columns and 'PV_Output' in df.columns, f\"Columns: {df.columns.tolist()}\"\n",
        "\n",
        "df['time'] = pd.to_datetime(df['time'], dayfirst=True, errors='coerce')\n",
        "df = df.dropna(subset=['time']).set_index('time').sort_index()\n",
        "\n",
        "df = df.resample('h').mean()"
      ],
      "metadata": {
        "id": "mG-tMRVWhy6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['hour'] = df.index.hour\n",
        "df['dayofyear'] = df.index.dayofyear\n",
        "\n",
        "\n",
        "df['hour_sin'] = np.sin(2*np.pi*df['hour']/24)\n",
        "df['hour_cos'] = np.cos(2*np.pi*df['hour']/24)\n",
        "df['doy_sin']  = np.sin(2*np.pi*df['dayofyear']/365.25)\n",
        "df['doy_cos']  = np.cos(2*np.pi*df['dayofyear']/365.25)\n",
        "\n",
        "base_feature_cols = [\n",
        "    'Irradiance','Temperature_C','Humidity','Cloud_Cover','Snowfall',\n",
        "    'hour_sin','hour_cos','doy_sin','doy_cos'\n",
        "]\n",
        "target_col = 'PV_Output'\n",
        "\n",
        "\n",
        "df[target_col] = df[target_col].interpolate(method='time')\n",
        "df[target_col] = df[target_col].clip(lower=0)\n",
        "\n",
        "for c in base_feature_cols:\n",
        "    if c in df.columns:\n",
        "        df[c] = df[c].interpolate(method='time')\n",
        "        df[c] = df[c].ffill().bfill()\n",
        "\n",
        "\n",
        "df['PV_lag1'] = df[target_col].shift(1)\n",
        "df = df.dropna(subset=['PV_lag1'])\n",
        "\n",
        "feature_cols = base_feature_cols + ['PV_lag1']\n",
        "\n",
        "assert np.isfinite(df[feature_cols + [target_col]].values).all(), \"Non-finite values remain after cleaning.\"\n",
        "\n",
        "print(\"Data range:\", df.index.min(), \"â†’\", df.index.max())\n",
        "print(\"Rows after cleaning:\", len(df))\n"
      ],
      "metadata": {
        "id": "CJKNXgzahy9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LEN = 24\n",
        "\n",
        "timestamps = df.index\n",
        "mask_train_all = timestamps.year <= 2023\n",
        "mask_test      = timestamps.year == 2024\n",
        "\n",
        "df_train_all = df.loc[mask_train_all].copy()\n",
        "df_test      = df.loc[mask_test].copy()\n",
        "\n",
        "n_train = len(df_train_all)\n",
        "val_tail = int(math.ceil(0.10 * n_train))\n",
        "df_train = df_train_all.iloc[:-val_tail].copy()\n",
        "df_val   = df_train_all.iloc[-val_tail:].copy()\n",
        "\n",
        "print(f\"Train hours: {len(df_train)}, Val hours: {len(df_val)}, Test hours: {len(df_test)}\")"
      ],
      "metadata": {
        "id": "JkN2DTh4hzAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zero_var_cols = df_train[feature_cols].std().replace({0.0: np.nan}).dropna().index\n",
        "zero_var_cols = [c for c in feature_cols if df_train[c].std() == 0.0]\n",
        "if zero_var_cols:\n",
        "    print(\"Dropping zero-variance cols:\", zero_var_cols)\n",
        "    feature_cols = [c for c in feature_cols if c not in zero_var_cols]\n",
        "\n",
        "X_scaler = StandardScaler()\n",
        "y_scaler = MinMaxScaler()\n",
        "\n",
        "X_train_f = X_scaler.fit_transform(df_train[feature_cols])\n",
        "X_val_f   = X_scaler.transform(df_val[feature_cols])\n",
        "X_test_f  = X_scaler.transform(df_test[feature_cols])\n",
        "\n",
        "y_train_f = y_scaler.fit_transform(df_train[[target_col]])\n",
        "y_val_f   = y_scaler.transform(df_val[[target_col]])\n",
        "y_test_f  = y_scaler.transform(df_test[[target_col]])\n",
        "\n",
        "for name, arr in [('X_train_f',X_train_f),('X_val_f',X_val_f),('X_test_f',X_test_f),\n",
        "                  ('y_train_f',y_train_f),('y_val_f',y_val_f),('y_test_f',y_test_f)]:\n",
        "    assert np.isfinite(arr).all(), f\"Non-finite values in {name}\"\n"
      ],
      "metadata": {
        "id": "7KikfYnOiD2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_sequences(X, y, seq_len):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - seq_len):\n",
        "        Xs.append(X[i:i+seq_len, :])\n",
        "        ys.append(y[i+seq_len, 0])\n",
        "    return np.array(Xs, dtype=np.float32), np.array(ys, dtype=np.float32)\n",
        "\n",
        "X_train, y_train = make_sequences(X_train_f, y_train_f, SEQ_LEN)\n",
        "X_val,   y_val   = make_sequences(X_val_f,   y_val_f,   SEQ_LEN)\n",
        "X_test,  y_test  = make_sequences(X_test_f,  y_test_f,  SEQ_LEN)\n",
        "\n",
        "print(\"Shapes | X_train:\", X_train.shape, \"X_val:\", X_val.shape, \"X_test:\", X_test.shape)\n",
        "\n",
        "for name, arr in [('X_train',X_train),('X_val',X_val),('X_test',X_test),\n",
        "                  ('y_train',y_train),('y_val',y_val),('y_test',y_test)]:\n",
        "    assert np.isfinite(arr).all(), f\"Non-finite values in {name}\""
      ],
      "metadata": {
        "id": "US0EyVMRiEAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_shifted = df_test[target_col].shift(1).values\n",
        "test_index = df_test.index[SEQ_LEN:]\n",
        "baseline_aligned = baseline_shifted[SEQ_LEN:]\n",
        "baseline_aligned = baseline_aligned[:len(y_test)]\n",
        "baseline_actual  = df_test[target_col].values[SEQ_LEN:SEQ_LEN+len(y_test)]"
      ],
      "metadata": {
        "id": "1Eh1nBtZiEJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Input(shape=(SEQ_LEN, X_train.shape[-1])),\n",
        "    Conv1D(filters=64, kernel_size=3, padding='causal', activation='relu'),\n",
        "    LayerNormalization(),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    Dropout(0.2),\n",
        "    Bidirectional(LSTM(32)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.1),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "optimizer = Adam(learning_rate=0.01, clipnorm=1.0)  # (or clipvalue=0.5)\n",
        "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "model.summary()\n",
        "\n",
        "es  = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True)\n",
        "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, min_lr=1e-5, verbose=1)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=200,\n",
        "    batch_size=128,\n",
        "    callbacks=[es, rlr,],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "ZfDo22O3iENL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_scaled = model.predict(X_test, verbose=0).reshape(-1, 1)\n",
        "y_pred = y_scaler.inverse_transform(y_pred_scaled).flatten()\n",
        "y_true = y_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "def rmse(a, b):\n",
        "    a = np.asarray(a, dtype=float)\n",
        "    b = np.asarray(b, dtype=float)\n",
        "    return np.sqrt(mean_squared_error(a, b))\n",
        "\n",
        "def mape(a, b, eps=1e-3):\n",
        "    a = np.asarray(a, dtype=float)\n",
        "    b = np.asarray(b, dtype=float)\n",
        "    mask = np.abs(a) > eps\n",
        "    if not np.any(mask):\n",
        "        return np.nan\n",
        "    return np.mean(np.abs((a[mask] - b[mask]) / a[mask])) * 100\n",
        "\n",
        "print(\"\\n=== Model vs Baseline (2024) ===\")\n",
        "print(f\"Model  RMSE: {rmse(y_true, y_pred):.4f} | \"\n",
        "      f\"MAE: {mean_absolute_error(y_true, y_pred):.4f} | \"\n",
        "      f\"MAPE: {mape(y_true, y_pred):.2f}%\")\n",
        "\n",
        "m_len = min(len(y_true), len(baseline_aligned))\n",
        "y_true_b = y_true[:m_len]\n",
        "base_b = baseline_aligned[:m_len]\n",
        "mask = np.isfinite(y_true_b) & np.isfinite(base_b)\n",
        "\n"
      ],
      "metadata": {
        "id": "506DVIlMiEQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_idx = test_index[:len(y_pred)]\n",
        "df_results = pd.DataFrame({\n",
        "    'datetime': results_idx,\n",
        "    'actual': y_true[:len(results_idx)],\n",
        "    'predicted': y_pred[:len(results_idx)]\n",
        "}).set_index('datetime')\n",
        "\n",
        "monthly = df_results.groupby([df_results.index.year, df_results.index.month]).apply(\n",
        "    lambda g: pd.Series({\n",
        "        'RMSE': rmse(g['actual'], g['predicted']),\n",
        "        'MAE': mean_absolute_error(g['actual'], g['predicted']),\n",
        "        'MAPE': mape(g['actual'], g['predicted'])\n",
        "    })\n",
        ")\n",
        "print(\"\\nMonthly metrics (year, month):\\n\", monthly)\n",
        "\n",
        "# January plot\n",
        "jan = df_results[df_results.index.month == 1]\n",
        "if not jan.empty:\n",
        "    plt.figure(figsize=(14,5))\n",
        "    plt.plot(jan.index, jan['actual'], label='Actual', linewidth=1)\n",
        "    plt.plot(jan.index, jan['predicted'], label='Predicted', alpha=0.9, linewidth=1)\n",
        "    plt.title('Forecast vs Actual â€” January 2024')\n",
        "    plt.xlabel('Date'); plt.ylabel('PV Output')\n",
        "    plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()"
      ],
      "metadata": {
        "id": "64jMFBzLicia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_daily = df_results.resample('D').sum(numeric_only=True)"
      ],
      "metadata": {
        "id": "J7IKJsGTkQ-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_daily[['actual','predicted']].plot(figsize=(12,8))\n",
        "df_daily"
      ],
      "metadata": {
        "id": "NomB_t5QkY9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "def calculate_error_metrics(y_true, y_pred):\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "\n",
        "    naive_forecast = np.roll(y_true, 1)[1:]\n",
        "    naive_error = np.mean(np.abs(y_true[1:] - naive_forecast))\n",
        "    mase = mae / naive_error if naive_error != 0 else np.inf\n",
        "\n",
        "    mask = y_true != 0\n",
        "    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "\n",
        "    return {\n",
        "        'MSE': mse,\n",
        "        'MAE': mae,\n",
        "        'MAPE': mape,\n",
        "        'MASE': mase\n",
        "    }"
      ],
      "metadata": {
        "id": "n0WgX8IfqD3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = calculate_error_metrics(df_results['actual'], df_results['predicted'])\n",
        "\n",
        "print(\"Error Metrics for model RNN:\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")"
      ],
      "metadata": {
        "id": "9jXg2yC4qG_I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}